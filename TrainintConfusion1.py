import csv
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# 1. Load the dataset manually with csv module
instruments = []
features = []

with open("Benzi1000_2^17ESTreiInstrumenteInregistrate.csv", 'r') as file:
    csv_reader = csv.reader(file)
    next(csv_reader)  # Skip header row
    for row in csv_reader:
        instruments.append(row[0])  # First column is instrument name
        # Convert band values to float, skipping instrument and filename columns
        band_values = [float(value) for value in row[2:]]
        features.append(band_values)

# Convert to numpy arrays
X = np.array(features, dtype=np.float32)
instruments = np.array(instruments)

# 2. Encode the instrument labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(instruments)
y_categorical = to_categorical(y_encoded)

# Print some information to verify data loading
print(f"Data shape: {X.shape}")
print(f"Unique instruments: {label_encoder.classes_}")
print(f"Number of classes: {len(label_encoder.classes_)}")

# 3. Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)

# 4. Reshape for Conv1D input
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# 5. Build Conv1D model
model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    
    Conv1D(128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),
    
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(len(label_encoder.classes_), activation='softmax')
])

# 6. Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 7. Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# 8. Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"\nâœ… Test Accuracy: {test_accuracy:.2f}")

# 9. Confusion Matrix
# Predict class probabilities
y_pred_probs = model.predict(X_test)
# Convert probabilities to class indices
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)

# Plot confusion matrix
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# Plot training & validation accuracy and loss
plt.figure(figsize=(12, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

plt.tight_layout()
plt.show()

# 10. Save the model
model.save("Model10ep_2^17_1000benzi.keras")